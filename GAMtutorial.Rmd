---
title: "GAM+ Workshop Tutorial"
output: html_document
date: '2022-10-27'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "~/Desktop/BGD_Repos/workshop_UseR18-master")
```

Doing [workshop video on GAMs and GAMLSS](https://www.youtube.com/watch?v=a6sTwkQGt3E) from R Consortium, with accompanying exercises from [github](https://github.com/mfasiolo/workshop_UseR18).

### Workshop prep described in README:

```{r load_packages}
if(!require('pacman')) {
  install.packages('pacman')
}
pacman::p_load(devtools, mgcViz, dplyr, gridExtra, gamlss, tidymv)

install_github("mfasiolo/mgcFam")
install.packages(c("languageR", "gamair", "e1071"))
```

Test install:

```{r test, eval=FALSE}
set.seed(2) ## simulate some data...
dat <- gamSim(2,n=500,dist="normal",scale=0.25)$data
b <- qgamV(y ~ s(x, z), data = dat, qu = 0.5)
plotRGL(sm(b, 1), residuals = TRUE)

# Fit GAM and get gamViz object
b <- mqgamV(y~s(x) + s(z) + I(x*z), data = dat, qu = c(0.2, 0.4, 0.6, 0.8),
aQgam = list(argGam = list(select = TRUE)), aViz = list("nsim" = 0))
# Either way, we all effects by doing
print(plot(b, allTerms = TRUE), pages = 1)
```

## Intro  to GAMs

### Additive modeling
In GAM - distribution of y isn't necessarily gaussian but only mean is modeled as function of covariates
- additive b/c sum of functions of j covariates ($f_j(x)$, where $f_j$'s can be smooth, fixed, or random with coefficients $\beta$), transformed given a link function (g).
- smooth effects - second function you plug parameters into ($f_2$) which is a sum of "basis" (known) spline functions (B-splines) times a $\beta$ that scales them. This $\beta$ is unknown, so GAM estimates it to get best result when you combine all the shifted B-splines linearly
- Other Types of Smooths:
  - other options are cubic regression splines (basis function has a different shape) - minimmize sum of squares of y AND curvature parameter (goodness of fit whie balancing linearity, not too curvy/overfit)
  - also cyclic cubic regression splines (y at minimum x = y at maximum x)
  - adaptive smooth - penalize curvature of fit differently depending where you are on x axis (keep fit more or less strictly linear in different parts)
- thin plate regression splines - for multidimensional data, isotropic (one smoothing parameter/penalty for all dimensions of predictor data (ie x1, x2, etc))
    - works if x1 and x2 have the same units (otherwise use tensor product smooth)
- tensor product smooth - smooth in 1 dimension (z) then let teh spline vary smoothy by x dimension
  - x and z are penalized separately (eg cyclic smooth on one, cubic regression on another)
  - **can use almost any kind of marginal?? what's a marginal in this context?**
- by-factor smooth - smooth effect will vary by factor level
  - can do 2 different ways, with different smoothing parameters (penalty) by level and/or different random intercepts for each level
For big data, save computational space by using `bam`
Fitting:
- chose rank r (number basis functions) that's big enough to be flexible BUT penalize regression coefficient $\beta$ to keep from overfitting (shrink towards 0)
- minimize penalized log likelihood (goodness of fit - complexity penalty)
Diagnostics:
- conditional residual checks - bin values, plot mean residual in each bin 
- effective degrees of freedom (EDF) - # parameters we're effectively using (k basis functions, but actually it's smaller b/c the penalty is shrinking things)
  - don't set too low, better to overestimate (default = 10)
  - see if your k is good by using conditional residuals check, increase k and see if model fit (AIC) improves, or `check(fit)` function:
    - if k' ~ edf -> might want to increase k
    - `gridCheck2D` - plot standardized residuals in each hexagon - want it to be random, if there's a patter -> try smoothing more

## Exercise Set 1

### 4. Mackerel egg data

```{r mackerel_load}
library(mgcViz); load("exercises/data/mack.rda"); load("exercises/data/coast.rda")
## plot data....
with(mack,plot(lon,lat,cex=0.2+egg.dens/150,col="red"))
lines(coast)
ind <- c(1,3,4,5,10,11,16)
pairs(mack[,ind])
```
"use gamV to fit a Poisson GAM with 1D smooth effects for all the variables involved, with the exceptions of net.area and log.net.area. Instead include in the model formula the term offset(log.net.area), meant to take into account the fact that the number of eggs captured is proportional to the net area"

```{r}
mack <- mack %>% mutate(log.net.area=log(net.area))
fit1 <- gamV(egg.count ~ s(b.depth, k=10) + s(lat, k=10) + s(lon, k=10) + s(temp.surf, k=10) + s(temp.20m, k=10) + s(c.dist, k=10) + offset(log.net.area),
             family = poisson(link="log"),
             data = mack)
#check residuals - simul2 seems like the computatinally cheap option vs simul1?
qq(fit1, method = "simul2")
```
Alright, this fit is real bad.

"Re-fit the models using a negative binomial (family=nb) or Tweedie (family=tw) response distribution, and check which model is better in terms of residuals QQ-plots and AIC."

```{r}
fit2 <- gamV(egg.count ~ s(b.depth, k=10) + s(lat, k=10) + s(lon, k=10) + s(temp.surf, k=10) + s(temp.20m, k=10) + s(c.dist, k=10) + offset(log.net.area),
             family = nb,
             data = mack)

fit3 <- gamV(egg.count ~ s(b.depth, k=10) + s(lat, k=10) + s(lon, k=10) + s(temp.surf, k=10) + s(temp.20m, k=10) + s(c.dist, k=10) + offset(log.net.area),
             family = tw,
             data = mack)

#check residuals
qq(fit2, method = "simul2")
qq(fit2, method = "simul2")

#check AIC
AIC(fit1, fit2, fit3)
```

Both fit2 and fit3 look better in terms of residuals and much better AIC.

"Use fit<-getViz(fit,nsim=50) to get some simulated residuals, and then use the check2D function with the l_gridCheck2D layer to look for residual patters across lon and lat." 

```{r}
#using fit3
fit3<-getViz(fit3,nsim=50) #simulating residuals
check2D(fit3, "lon", "lat") + l_gridCheck2D() #bw=bin width, default to 1/20th the ranges of x1 and x2
```
"Then refit the model using a bivariate isotropic effect s(lon, lat, k=100), re-check the residuals and see whether AIC has improved."

```{r}
fit4 <- gamV(egg.count ~ s(b.depth, k=10) + s(lat, lon, k=100) + s(temp.surf, k=10) + s(temp.20m, k=10) + s(c.dist, k=10) + offset(log.net.area),
             family = tw,
             data = mack)

#recheck residuals
qq(fit4, method = "simul2")
fit4<-getViz(fit4,nsim=50) #simulating residuals
check2D(fit4, "lon", "lat") + l_gridCheck2D() #bw=bin width, default to 1/20th the ranges of x1 and x2

#check fit
AIC(fit4)
```

Not actually sure which residual pattern is better? But AIC has improved, as has QQ plotted residuals.
**Question: how are the residuals plotted in the QQ plot calculated? Are they overall (like 3D, residuals from all dimensions) vs 2D just maps how far a point deviates from one of several planes?**

"Use check to verify whether the number of basis functions used for the smooth effects is sufficiently large. Then use the check1D function with the l_gridCheck1D layer look for residual patterns across some of the variables. If necessary, modify the model."

```{r}
check.gamViz(fit4)

#lets check out some 1D residuals
p1 <- (check1D(fit4, "temp.surf") + l_gridCheck1D(mean))$ggObj
p2 <- (check1D(fit4, "temp.20m") + l_gridCheck1D(mean))$ggObj
p3 <- (check1D(fit4, "b.depth") + l_gridCheck1D(mean))$ggObj
p4 <- (check1D(fit4, "c.dist") + l_gridCheck1D(mean))$ggObj

grid.arrange(grobs = list(p1, p2, p3, p4), ncol = 2)
```

Let's make some adjustments, starting with raising k based on check.gamViz outputs

```{r}
#adjust temp.20m k
fit5 <- gamV(egg.count ~ s(b.depth, k=10) + s(lat, lon, k=100) + s(temp.surf, k=10) + s(temp.20m, k=20) + s(c.dist, k=10) + offset(log.net.area),
             family = tw,
             data = mack)

check.gamViz(fit5)

#check residuals
fit5<-getViz(fit5,nsim=50) #simulating residuals
check1D(fit5, "temp.20m") + l_gridCheck1D(mean)
check1D(fit5, "c.dist") + l_gridCheck1D(mean)
```
*Ok, fit definitely isn't good but i'm not sure what to do other than adjusting k...*
**Question: what is the linear predictor being plotted in the bottom left quadrant?**

"Plot the fitted effects using plot. Which effects look more important (look at the scales)? Use the plotRGL function to manipulate spatial effect interactively."

```{r}
plot(fit5)

tmp <- sm(fit5, 2)
plotRGL(tmp, residuals = TRUE)
```

### 5. Bone mineral density modelling

```{r}
load("exercises/data/calcium.rda")
```

"use gamV to fit a Gaussian GAM model with bmd as response and linear effects for age and group. In the call to gamV set the argument aViz=list(nsim = 50) to have some simulated responses for residuals checks. Use summary to print the model output. Is the placebo effect significant?"

**Question: why do we keep simulating the residuals? Is it because the data isn't necessarily evenly distributed across all the predictors?**

```{r}
fit.bmd1 <- gamV(bmd ~ age + group, 
             family = gaussian,
             data = calcium,
             aViz = list(nsim = 50))
summary(fit.bmd1)
```

Based on this model, the placebo group does have significantly (slightly) lower BMD than the treatment group.

"Use check1D with the l_gridCheck1D layer to check that the mean of the negative residuals does not depart too much from 0, for any of the subjects."

```{r}
check1D(fit.bmd1, "age") + l_gridCheck1D(mean)
check1D(fit.bmd1, "group") + l_gridCheck1D(mean)

#from solution - can check residuals on measure that isn't in the model
check1D(fit.bmd1, calcium$person) + l_gridCheck1D(mean)
```
**Question: why are the red CIs sometimes dashes and sometimes crosses? Is it continuous v categorical data?**

Honestly, based on these plots the residuals look pretty good, but I'll work through the rest of this anyway.

"If you see significant departures add a random effect for person to the models formula (s(person, bs="re")), then re-fit and re-check the residuals. Print the model output again using summary."

Note: `bs="re"` in a smooth term denotes a random effect penalized with a ridge penalty. More info [here](https://stat.ethz.ch/R-manual/R-devel/library/mgcv/html/smooth.terms.html).

```{r}
#adding smooth effect for individual
fit.bmd2 <- gamV(bmd ~ age + group + s(person, bs="re"), 
             family = gaussian,
             data = calcium,
             aViz = list(nsim = 50))
summary(fit.bmd2)

#recheck residuals
bmd.p1 <- (check1D(fit.bmd2, "age") + l_gridCheck1D(mean))$ggObj
bmd.p2 <- (check1D(fit.bmd2, "group") + l_gridCheck1D(mean))$ggObj
bmd.p3 <- (check1D(fit.bmd2, "person") + l_gridCheck1D(mean))$ggObj

grid.arrange(grobs = list(bmd.p1, bmd.p2, bmd.p3), ncol = 2)
```

Ok, confidence intervals are definitely tighter now.

"Now modify the model formula to use a smooth effect for age, and plot the fitted effects using plot. Use the function AIC to compare the model with a smooth effects for age with the model which uses a linear age effect."

```{r}
#adding smooth effect for age
fit.bmd3 <- gamV(bmd ~ s(age) + group + s(person, bs="re"), 
             family = gaussian,
             data = calcium,
             aViz = list(nsim = 50))
summary(fit.bmd3)

#plot fitted effects
plot(fit.bmd3, allTerms = T) #allTerms -> also plot group (not just smooth effects)

#compare linear vs smoothed age models
AIC(fit.bmd2, fit.bmd3)
```

"Verify whether the smooth age effect is different between the placebo and the treatment group, by using a by-factor smooth. To do this substitute s(age) with s(age, by=group) in the model formula, refit and then plot the fitted effects. To see the difference between the two smooths more clearly, use the plotDiff function with the l_fitLine and l_ciLine layers."

`l_fitLine`: adds lines for 1 parametric effect, 1 group of parametric effects, or 1 smooth 1D effect
`l_ciLine`: adds CIs

```{r}
#bifactor smooth effect
fit.bmd4 <- gamV(bmd ~ s(age, by=group) + s(person, bs="re"), 
             family = gaussian,
             data = calcium,
             aViz = list(nsim = 50))
summary(fit.bmd4)

#plot fitted effects
plot(fit.bmd4)

#plotdiff
plotDiff(s1 = sm(fit.bmd4, 1), s2 = sm(fit.bmd4, 2)) + #takes difference in smooth effects at different levels
  l_ciPoly() + 
  l_fitLine()
```

**Question: ok, but how exactly do I interpret this figure?** Per solution: "BMD increases faster in the group taking the supplement, and the effect might be levelling off after 12 years of age."

##Beyond mean modelling: GAMLSS and quantile GAMs

###Intro to GAMLSS
GAM models conditional distribution of y by modeling expected value of y given x
scale (variance) and shape can change with x's
P Additive Models, 1 for each parameter (with own link functions)

###Quantile GAMs
y is usually continuous, need to find CDF of y given x (inversion of CDF -> quantile)
global structure keeps quantile lines from crossing across GAMLSS
can just model each quantile's mean directly/individually instead of getting global gaussian distribution model that will keep them all in line (each quantile modeled independently from the others)
- minimize "pinball" loss + penalty
- can look at median instead of mean - more resistant to outliers
- may be interested in certain specific quantiles (e.g. only care about 95th percentile of your data, like the peak rainfall you can expect in an environment)
- same explanatory variable may have different shapes/effects in different quantiles!
use `qgam` package which has a smoothed version of the pinball loss (extended log-F)
- modify `err` parameter - bigger -> bigger, more stable but more biased
Diagnostics:
- check proportion of residuals that are negative, should be ~ the proportion of the quantile you're modeling (e.g. if you're modelling the 75th percentile, about 75% of your residuals should be < 0)

**Note:** Big Data (`bam`) models don't work with GAMLSS or quantile GAMs

## Exercise Set 2

### 1. GAMLSS modelling of aggregate UK electricity demand

### 3. Solar production modelling

### 4. Body Mass Index (BMI) of Dutch boys

"The use gamV to fit a Gaussian GAM with simply a single smooth effect for age. Set argument aViz=list(nsim = 50) to have some simulated responses for residuals checks. Then plot the data (a scatterplot bmi vs age) and add a line representing the fitted mean BMI (you can use the predict function)."

```{r}
load("exercises/data/dbbmi.rda")

fit.bmi1 <- gamV(bmi ~ s(age), 
             family = gaussian,
             data = dbbmi,
             aViz = list(nsim = 50))
summary(fit.bmi1)

#plot data with line to predict
dbbmi$gam.fit <-  (predict(fit.bmi1, se = TRUE))$fit #get mean prediction
dbbmi$gam.fit.se <-  (predict(fit.bmi1, se = TRUE))$se.fit #get SE for Ci's

ggplot(dbbmi, aes(x = age, y = bmi) ) +
  geom_point(color="lightgreen") +
  geom_ribbon(aes(ymin = (gam.fit - 2*gam.fit.se), ymax = (gam.fit + 2*gam.fit.se)), fill="black", alpha = .3) +
  geom_line(aes(y = gam.fit), size = 1)
```

"Check the residual distribution using qq: do you see any problem? Then use the check1D function together with the l_gridCheck1D(gridFun=sd) layer to check whether the conditional standard deviation of the residuals varies with age."

```{r}
#gam residuals
qq(fit.bmi1, method = "simul2")

#plot residuals mean & SDs
check1D(fit.bmi1, "age") + l_gridCheck1D(mean) 
check1D(fit.bmi1, "age") + l_gridCheck1D(gridFun=sd)
```

Variance changes A LOT with age!

"If so address this by fitting a Gaussian GAMLSS model (family = gaulss), with model formula list(bmi ~ s(age), ~ s(age)). Then repeat the residuals checks. Any improvement?"

```{r}
fit.bmi2 <- gamV(list(bmi ~ s(age), ~ s(age)), 
             family = gaulss,
             data = dbbmi,
             aViz = list(nsim = 50))

#recheck residuals
check1D(fit.bmi2, "age") + l_gridCheck1D(mean)
check1D(fit.bmi2, "age") + l_gridCheck1D(gridFun=sd)
```

All residuals are better! Hurrah!

"Use check to verify whether the number of basis functions used for the smooth effects is sufficiently large. Then increase the number of basis functions used for each effect to 20 (k=20), and use an adaptive basis fom the effect of age on mean BMI (bs = "ad")."

```{r}
check(fit.bmi2)

#refitting
fit.bmi3 <- gamV(list(bmi ~ s(age, k=20, bs="ad"), ~ s(age, k=20, bs="ad")), 
             family = gaulss,
             data = dbbmi,
             aViz = list(nsim = 50))
check(fit.bmi3)

#plotting for fun
pred <- predict(fit.bmi3, what="mu", type="response")
names(pred)
pred

pred.all <- gamlss::predictAll(fit.bmi3)

dbbmi$gamlss.fit <-  (predict(fit.bmi3, se = TRUE))$fit #get mean prediction
dbbmi$gamlss.fit.se.low <-  (predict(fit.bmi3, se = TRUE))$se.fit #get SE for Ci's

ggplot(dbbmi, aes(x = age, y = bmi) ) +
  geom_point(color="lightblue") +
  geom_ribbon(aes(ymin = (gamlss.fit - 2*gamlss.fit.se), ymax = (gamlss.fit + 2*gamlss.fit.se)), fill="black", alpha = .3) +
  geom_line(aes(y = gamlss.fit), size = 1)
```

### 5. Rent modelling in Munich


# Playing Around
Messing around with some data/visuals for QNC class presentation

```{r}
#making a fake stage variable for DB BMI dataset
fake.stage.list <- c("A","B","C","D","E")
dbbmi <- dbbmi %>% mutate(sim.stage = case_when(
                  age < 11 ~ sample(x=fake.stage.list, size=7294, replace=TRUE, prob=c(0.5, 0.25, 0.2, 0.05, 0)), 
                  age >= 11 & age < 14 ~ sample(x=fake.stage.list, size=7294, replace=TRUE, prob=c(0.25, 0.5, 0.15, 0.05, 0.05)),
                  age >= 14 & age < 16 ~ sample(x=fake.stage.list, size=7294, replace=TRUE, prob=c(0, 0.2, 0.5, 0.2, 0.1)),
                  age >= 16 & age < 18 ~ sample(x=fake.stage.list, size=7294, replace=TRUE, prob=c(0, 0.1, 0.2, 0.5, 0.2)),
                  age >= 18 ~ "E"))

```

```{r}
#OK, let's make a GAM
fit.bmi.sim <- gamV(bmi ~ s(age, by=as.factor(sim.stage), k=20, bs="ad"), 
             family = gaussian,
             data = dbbmi,
             aViz = list(nsim = 50))
check(fit.bmi.sim)

#predictions
dbbmi$sim.gam.fit <-  (predict(fit.bmi.sim, se = TRUE))$fit #get mean prediction
dbbmi$sim.gam.fit.se <- (predict(fit.bmi.sim, se = TRUE))$se.fit #get SE for Ci's

#plot test
opt1 <- ggplot(dbbmi, aes(x = age, y = bmi) ) +
  geom_point(aes(color=sim.stage), alpha=.2) +
  #geom_ribbon(aes(ymin = (sim.gam.fit - 2*sim.gam.fit.se), ymax = (sim.gam.fit + 2*sim.gam.fit.se)), fill="black", alpha = .3) +
  geom_line(data=subset(dbbmi, sim.stage=="A"),aes(y = sim.gam.fit, color=sim.stage), linetype="dashed", size = 1) +
  geom_line(data=subset(dbbmi, sim.stage=="B"),aes(y = sim.gam.fit, color=sim.stage), linetype="dashed", size = 1) +
  geom_line(data=subset(dbbmi, sim.stage=="C"),aes(y = sim.gam.fit, color=sim.stage), linetype="dashed", size = 1) +
  geom_line(data=subset(dbbmi, sim.stage=="D"),aes(y = sim.gam.fit, color=sim.stage), linetype="dashed", size = 1) +
  geom_line(data=subset(dbbmi, sim.stage=="E"),aes(y = sim.gam.fit, color=sim.stage), linetype="dashed", size = 1) +
  ylab("phenotype") +
  theme_bw()

#save
ggsave(
  "test_plot1.png", height=8, width=15, units="in",
  plot = last_plot())

#plot test 2
opt2 <- ggplot(dbbmi, aes(x = age, y = bmi) ) +
  geom_point(aes(color=sim.stage), alpha=.2) +
  #geom_ribbon(aes(ymin = (sim.gam.fit - 2*sim.gam.fit.se), ymax = (sim.gam.fit + 2*sim.gam.fit.se)), fill="black", alpha = .3) +
  geom_line(data=subset(dbbmi, sim.stage=="A"),aes(y = sim.gam.fit), size = 1) +
  geom_line(data=subset(dbbmi, sim.stage=="B"),aes(y = sim.gam.fit), size = 1) +
  geom_line(data=subset(dbbmi, sim.stage=="C"),aes(y = sim.gam.fit), size = 1) +
  geom_line(data=subset(dbbmi, sim.stage=="D"),aes(y = sim.gam.fit), size = 1) +
  geom_line(data=subset(dbbmi, sim.stage=="E"),aes(y = sim.gam.fit), size = 1) +
  ylab("phenotype") +
  theme_bw() +
  facet_wrap(~sim.stage, ncol = 1)

#save
ggsave(
  "test_plot2.png", height=20, width=20, units="in",
  plot = last_plot())
```
Hmm, dont think thats working. Lets try the gamlss package

```{r}
bmi.gamlss.sim <-gamlss(bmi~pb(age, by=sim.stage), family = GA, data=dbbmi, trace = FALSE)
summary(bmi.gamlss.sim)

#hopefully adding fitted vals
dbbmi <- dbbmi %>% mutate(sim.fit = fitted(bmi.gamlss.sim))
pred <- predict_gam(bmi.gamlss.sim)
summary(dbbmi)

#plot
dbbmi %>% ggplot(aes(y=fit, x=age)) +  
    geom_point(aes(x=age, y=bmi, color = sim.stage, shape=sim.stage), alpha=.6) +
    geom_smooth_ci()

  

plot(bmi~age, col = fake.stage, data = dbbmi)
lines(fitted(bmi.gamlss.sim)[order(dbbmi$age)]~dbbmi$age[order(dbbmi$age)])

#plot
gam.plots <- list()
for (stage in fake.stage)
  {
  plot.test<- ggplot(data=subset(dbbmi, fake.stage==stage),aes(y=bmi, x=age))+ 
    geom_point(fill="lightgreen")
  #lines(fitted(bmi.gamlss.sim)[order(dbbmi$age)]~dbbmi$age[order(dbbmi$age)])
  gam.plots[[stage]] <- plot.test
}
gam.plots
```

## Book Tutorials
Playing with RS, CG and mixed optimization methods

```{r}
system.time(capture.output(m1 <- gamlss(fish~log(lake), sigma.fo=~log(lake), family=PIG, data=species)))
system.time(capture.output(m2 <- gamlss(fish~log(lake), sigma.fo=~log(lake), family=PIG, data=species, method=CG(), n.cyc=100)))
system.time(capture.output(m3 <- gamlss(fish~log(lake), sigma.fo=~log(lake), family=PIG, data=species, method=mixed(1,100))))
```


```{r}
summary(m1)
summary(m2)
summary(m3)

wp(m1)
wp(m2)
wp(m3)

m1$sbc
m2$sbc
m3$sbc

GAIC(m1, m2, m3, k=2)
GAIC(m1, m2, m3, k=3)
GAIC(m1, m2, m3, k=4)
GAIC(m1, m2, m3, k=log(nrow(species)))
```

```{r}
m1.rs <- gamlss(OILPRICE~pb(respLAG) + pb(HO1_log),
                sigma.formula = ~pb(respLAG) + pb(HO1_log),
                nu.formula = ~pb(respLAG) + pb(HO1_log),
                tau.formula = ~pb(respLAG) + pb(HO1_log),
                family=SHASHo, data=oil, method=RS(20))
m1.mx <- gamlss(OILPRICE~pb(respLAG) + pb(HO1_log),
                sigma.formula = ~pb(respLAG) + pb(HO1_log),
                nu.formula = ~pb(respLAG) + pb(HO1_log),
                tau.formula = ~pb(respLAG) + pb(HO1_log),
                family=SHASHo, data=oil, method=mixed(10, 10), gd.tol=Inf)

GAIC(m1.rs, m1.mx, k=2)
GAIC(m1.rs, m1.mx, k=3)
GAIC(m1.rs, m1.mx, k=4)
GAIC(m1.rs, m1.mx, k=log(nrow(oil)))
```
